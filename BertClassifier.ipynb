{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install bert package for tensorflow v1\n",
    "!pip install bert-tensorflow==1.0.1\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "from datetime import datetime\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm.notebook import tqdm #adds progress bars to show loop status\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class BERTClasiffier(object):\n",
    "\n",
    "    \"\"\"This class consists of functions to create, build, intialize and train the fine-tuned bert model as well as evaluate the model and \n",
    "    make predictions on a test set.\n",
    "    Order of execution from top to bottom -\n",
    "    GetPrediction -> evaluate -> train -> InitializeModel(internally called by train function) -> ModelFnBuilder (internally called\n",
    "    by InitializeModel function) -> CreateModel (internally called by ModelFnBuilder function)\"\"\"\n",
    "\n",
    "    def __init__(self, data, config, input):\n",
    "        self.data = data\n",
    "        self.config = config\n",
    "        self.input = input\n",
    "\n",
    "    def CreateModel(self, is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                     num_labels, BERT_MODEL_HUB):\n",
    "  \n",
    "        \"\"\"Create the base model and add fine-tuning layers to the model\"\"\"\n",
    "\n",
    "        bert_module = hub.Module(\n",
    "            BERT_MODEL_HUB,\n",
    "            trainable=True)\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            segment_ids=segment_ids)\n",
    "        bert_outputs = bert_module(\n",
    "            inputs=bert_inputs,\n",
    "            signature=\"tokens\",\n",
    "            as_dict=True)\n",
    "\n",
    "        # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "        # Use \"sequence_outputs\" for token-level output.\n",
    "        output_layer = bert_outputs[\"pooled_output\"]\n",
    "        # with tf.Session() as sess:\n",
    "        output_layer1 = bert_outputs[\"pooled_output\"]\n",
    "        # output_layer1 = 999\n",
    "        hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "        # Create our own layer to tune for data.\n",
    "        output_weights = tf.get_variable(\n",
    "            \"output_weights\", [num_labels, hidden_size],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "\n",
    "            # Dropout helps prevent overfitting\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n",
    "\n",
    "            logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "            # Convert labels into one-hot encoding\n",
    "            one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "            predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "            # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "            if is_predicting:\n",
    "                return (predicted_labels, log_probs, output_layer1)\n",
    "\n",
    "            # If we're train/eval, compute loss between predicted and actual label\n",
    "            per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "            loss = tf.reduce_mean(per_example_loss)\n",
    "            return (loss, predicted_labels, log_probs)\n",
    "\n",
    "    def ModelFnBuilder(self, num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps, BERT_MODEL_HUB):\n",
    "        \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "        def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "            \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "            input_ids = features[\"input_ids\"]\n",
    "            input_mask = features[\"input_mask\"]\n",
    "            segment_ids = features[\"segment_ids\"]\n",
    "            label_ids = features[\"label_ids\"]\n",
    "\n",
    "            is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "            # TRAIN and EVAL\n",
    "            if not is_predicting:\n",
    "\n",
    "                (loss, predicted_labels, log_probs) = self.CreateModel(\n",
    "                    is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels, BERT_MODEL_HUB)\n",
    "\n",
    "                train_op = bert.optimization.create_optimizer(\n",
    "                    loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "                # Calculate evaluation metrics. \n",
    "                def metric_fn(label_ids, predicted_labels):\n",
    "                    accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                    true_pos = tf.metrics.true_positives(\n",
    "                            label_ids,\n",
    "                            predicted_labels)\n",
    "                    true_neg = tf.metrics.true_negatives(\n",
    "                            label_ids,\n",
    "                            predicted_labels)   \n",
    "                    false_pos = tf.metrics.false_positives(\n",
    "                            label_ids,\n",
    "                            predicted_labels)  \n",
    "                    false_neg = tf.metrics.false_negatives(\n",
    "                            label_ids,\n",
    "                            predicted_labels)\n",
    "        \n",
    "                    return {\n",
    "                        \"eval_accuracy\": accuracy,\n",
    "                        \"true_positives\": true_pos,\n",
    "                        \"true_negatives\": true_neg,\n",
    "                        \"false_positives\": false_pos,\n",
    "                        \"false_negatives\": false_neg,\n",
    "                        }\n",
    "\n",
    "                eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "                if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                        loss=loss,\n",
    "                        train_op=train_op)\n",
    "                else:\n",
    "                    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                        loss=loss,\n",
    "                        eval_metric_ops=eval_metrics)\n",
    "            else:\n",
    "                (predicted_labels, log_probs, output_layer) = self.CreateModel(\n",
    "                    is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels, BERT_MODEL_HUB)\n",
    "                predictions = {\n",
    "                              'probabilities': log_probs,\n",
    "                              'labels': predicted_labels,\n",
    "                              'pooled_output': output_layer\n",
    "                              }\n",
    "                return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "        # Return the actual model function in the closure\n",
    "        return model_fn\n",
    "\n",
    "    def InitializeModel(self, BERT_MODEL_HUB):\n",
    "        batchSize = self.config.training.batchSize\n",
    "        learningRate = self.config.training.learningRate\n",
    "        numTrainEpochs = self.config.training.numTrainEpochs\n",
    "        # Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n",
    "        warmUpProportion = self.config.training.warmUpProportion\n",
    "        # Model configs\n",
    "        SAVE_CHECKPOINTS_STEPS = 300\n",
    "        SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "        # Compute train and warmup steps from batch size\n",
    "        self.num_train_steps = int(len(self.input.train_features) / batchSize * numTrainEpochs)\n",
    "        self.num_warmup_steps = int(self.num_train_steps * warmUpProportion)\n",
    "\n",
    "        # Specify output directory and number of checkpoint steps to save\n",
    "        run_config = tf.estimator.RunConfig(\n",
    "            model_dir= self.config.checkpointPath,\n",
    "            save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "            save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "        \n",
    "        #Initializing the model and the estimator\n",
    "        self.model_fn = self.ModelFnBuilder(\n",
    "            num_labels=len(self.config.labelList),\n",
    "            learning_rate=learningRate,\n",
    "            num_train_steps=self.num_train_steps,\n",
    "            num_warmup_steps=self.num_warmup_steps,\n",
    "            BERT_MODEL_HUB = BERT_MODEL_HUB)\n",
    "\n",
    "        self.estimator = tf.estimator.Estimator(\n",
    "            model_fn=self.model_fn,\n",
    "            config=run_config,\n",
    "            params={\"batch_size\": batchSize})\n",
    "        \n",
    "    def train(self, BERT_MODEL_HUB):\n",
    "        #Training the model\n",
    "        print(f'Beginning Training!')\n",
    "        current_time = datetime.now()\n",
    "        self.InitializeModel(BERT_MODEL_HUB)\n",
    "        self.estimator.train(input_fn=self.input.train_input_fn, max_steps=self.num_train_steps)\n",
    "        print(\"Training took time \", datetime.now() - current_time)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        #Evaluating the model with Validation set\n",
    "        self.estimator.evaluate(input_fn=self.input.val_input_fn, steps=None)\n",
    "\n",
    "    def GetPrediction(self, in_sentences, type_output = \"features\"):\n",
    "        # A method to get predictions\n",
    "\n",
    "        #A list to map the actual labels to the predictions\n",
    "        labels = self.config.labelList\n",
    "        input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n",
    "        input_features = run_classifier.convert_examples_to_features(input_examples, self.config.labelList, self.config.maxSeqLength, self.input.tokenizer)\n",
    "        #Predicting the classes \n",
    "        predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=self.config.maxSeqLength, is_training=False, drop_remainder=False)\n",
    "        predictions = self.estimator.predict(predict_input_fn)\n",
    "        print(predictions)\n",
    "        if type_output == \"features\":\n",
    "            return [prediction['pooled_output'] for _,prediction in enumerate(predictions) ]\n",
    "        else:\n",
    "            return ([(sentence, prediction['probabilities'],\n",
    "                prediction['labels'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
