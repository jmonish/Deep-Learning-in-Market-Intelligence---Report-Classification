{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install bert package for tensorflow v1\n",
    "!pip install bert-tensorflow==1.0.1\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "from datetime import datetime\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm.notebook import tqdm #adds progress bars to show loop status\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class GenerateBertFeatures(object):\n",
    "\n",
    "    \"\"\"BERT cannot consume texts as is in an alphabetic format. Thus, we need to convert input texts to a format which BERT understands. \n",
    "    This class consists of functions which will be used to convert the inout texts first into input examples and then into input features.\n",
    "    Order of execution from top to bottom -\n",
    "    GetFeatures -> CreateTokenizerFromHubModule -> GetInputExamples\"\"\"\n",
    "\n",
    "    def __init__(self, data, config):\n",
    "        self.config = config\n",
    "        self.data = data\n",
    "\n",
    "    def GetInputExamples(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This function converts the input texts into inout examples which conists of 4 different entities for every input:\n",
    "        guid: Unique id for the example.\n",
    "        text_a: String data. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified.\n",
    "        text_b: (Optional) string data. The untokenized text of the second sequence. Only must be specified for sequence pair tasks.\n",
    "        label: String data. The label of the example. This should be specified for train and evaluate examples, but not for test examples.\n",
    "        \"\"\"\n",
    "\n",
    "        DATA_COLUMN = \"text\"\n",
    "        LABEL_COLUMN = \"label\"\n",
    "        train_InputExamples = self.data.train_df.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "    \n",
    "        val_InputExamples = self.data.test_df.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "        \n",
    "        return train_InputExamples, val_InputExamples\n",
    "\n",
    "    def CreateTokenizerFromHubModule(self, BERT_MODEL_HUB):\n",
    "\n",
    "        \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "            tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "            with tf.Session() as sess:\n",
    "                vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "        return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "    def GetFeatures(self, BERT_MODEL_HUB):\n",
    "\n",
    "        \"\"\"This function takes input from CreateTokenizerFromHubModule() and GetInputExamples() and then converts the input examples into \n",
    "        BERT features. Parameter is the bert model downloaded at the very beginning of this notebook.\"\"\"\n",
    "\n",
    "        self.tokenizer = self.CreateTokenizerFromHubModule(BERT_MODEL_HUB)\n",
    "        train_InputExamples, val_InputExamples = self.GetInputExamples()\n",
    "        self.train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, self.config.labelList, self.config.maxSeqLength, self.tokenizer)\n",
    "        self.val_features = bert.run_classifier.convert_examples_to_features(val_InputExamples, self.config.labelList, self.config.maxSeqLength, self.tokenizer)\n",
    "\n",
    "        # Create an input function for training. drop_remainder = True for using TPUs.\n",
    "        self.train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "            features=self.train_features,\n",
    "            seq_length=self.config.maxSeqLength,\n",
    "            is_training=True,\n",
    "            drop_remainder=False)\n",
    "\n",
    "        # Create an input function for validating. drop_remainder = True for using TPUs.\n",
    "        self.val_input_fn = run_classifier.input_fn_builder(\n",
    "            features=self.val_features,\n",
    "            seq_length=self.config.maxSeqLength,\n",
    "            is_training=False,\n",
    "            drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
