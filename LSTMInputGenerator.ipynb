{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install bert package for tensorflow v1\n",
    "!pip install bert-tensorflow==1.0.1\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "from datetime import datetime\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm.notebook import tqdm #adds progress bars to show loop status\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Generator(object):\n",
    "\n",
    "    \"\"\"This class consists of functions to convert the training, validation and test datasets into a format acceptable by LSTM model. \n",
    "    LSTM takes inputs with fixed width only. But the vector representations of every report are of variable length as different reports have\n",
    "    different number of words and thus different number of text splits. Each generator function takes batches of given size, gets the size of \n",
    "    the largest input and extends the remaining inputs to the size of the largest, filling them with a special value. This process is along all \n",
    "    the data. This way, all batches sequences would have the same length. \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def train_generator(self, df):\n",
    "        num_sequences = len(df['emb'].to_list())\n",
    "        batch_size = self.config.training.batch_size_train\n",
    "        batches_per_epoch = self.config.training.batches_per_epoch_train\n",
    "\n",
    "        #make sure that all input data passes throught training\n",
    "        assert batch_size * batches_per_epoch == num_sequences   \n",
    "\n",
    "        num_features= 768\n",
    "        x_list= df['emb'].to_list()\n",
    "        y_list =  df.label.to_list()\n",
    "        # Generate batches\n",
    "        while True:\n",
    "            for b in range(batches_per_epoch):\n",
    "                longest_index = (b + 1) * batch_size - 1\n",
    "                timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "                x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "                y_train = np.zeros((batch_size,  1))\n",
    "                for i in range(batch_size):\n",
    "                    li = b * batch_size + i\n",
    "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                    y_train[i] = y_list[li]\n",
    "                yield x_train, y_train\n",
    "\n",
    "    def val_generator(self, df):\n",
    "        num_sequences = len(df['emb'].to_list())\n",
    "        batch_size = self.config.training.batch_size_val\n",
    "        batches_per_epoch = self.config.training.batches_per_epoch_val\n",
    "\n",
    "        #make sure that all input data passes throught training\n",
    "        assert batch_size * batches_per_epoch == num_sequences\n",
    "\n",
    "        num_features= 768\n",
    "        x_list= df['emb'].to_list()\n",
    "        y_list =  df.label.to_list()\n",
    "        # Generate batches\n",
    "        while True:\n",
    "            for b in range(batches_per_epoch):\n",
    "                longest_index = (b + 1) * batch_size - 1\n",
    "                timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-31:], key=len))\n",
    "                x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "                y_train = np.zeros((batch_size,  1))\n",
    "                for i in range(batch_size):\n",
    "                    li = b * batch_size + i\n",
    "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                    y_train[i] = y_list[li]\n",
    "                yield x_train, y_train\n",
    "\n",
    "    def test_generator(self, df):\n",
    "        num_sequences = len(df['emb'].to_list())\n",
    "        batch_size = self.config.training.batch_size_test\n",
    "        batches_per_epoch = self.config.training.batches_per_epoch_test\n",
    "\n",
    "        #make sure that all input data passes throught training\n",
    "        assert batch_size * batches_per_epoch == num_sequences\n",
    "        \n",
    "        num_features= 768\n",
    "        x_list= df['emb'].to_list()\n",
    "        y_list =  df.label.to_list()\n",
    "        # Generate batches\n",
    "        while True:\n",
    "            for b in range(batches_per_epoch):\n",
    "                longest_index = (b + 1) * batch_size - 1\n",
    "                timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-31:], key=len))\n",
    "                x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "                y_train = np.zeros((batch_size,  1))\n",
    "                for i in range(batch_size):\n",
    "                    li = b * batch_size + i\n",
    "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                    y_train[i] = y_list[li]\n",
    "                yield x_train, y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
